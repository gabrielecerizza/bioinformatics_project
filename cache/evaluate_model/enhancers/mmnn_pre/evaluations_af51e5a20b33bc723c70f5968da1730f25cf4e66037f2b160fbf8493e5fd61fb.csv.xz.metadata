{
    "creation_time": 1623806558.478363,
    "creation_time_human": "2021-06-16 03:22:38",
    "time_delta": 113.83819723129272,
    "time_delta_human": "1 minute and 53 seconds",
    "file_dump_time": 0.007880926132202148,
    "file_dump_time_human": "0 seconds",
    "file_dump_size": 936,
    "file_dump_size_human": "936 Bytes",
    "load_kwargs": {},
    "dump_kwargs": {},
    "function_name": "evaluate_model",
    "function_file": "F:\\Copia HD G\\Universita\\UNIMI\\Bioinformatics\\project2\\bioproject\\model_evaluation.py:25",
    "args_to_ignore": [
        "model",
        "train_sequence",
        "valid_sequence",
        "test_sequence"
    ],
    "source": "@Cache(\n    cache_path=[\n        \"cache/{function_name}/{region}/{model_name}/\"\n        + \"history_{_hash}.csv.xz\",\n        \"cache/{function_name}/{region}/{model_name}/\"\n        + \"evaluations_{_hash}.csv.xz\",\n    ],\n    args_to_ignore=[\n        \"model\", \"train_sequence\", \"valid_sequence\", \"test_sequence\"\n    ]\n)\ndef evaluate_model(\n        model: Model,\n        model_name: str,\n        region: str,\n        train_sequence: MixedSequence,\n        valid_sequence: MixedSequence,\n        test_sequence: MixedSequence,\n        holdout_number: int,\n        use_feature_selection: bool,\n        use_validation_set: bool,\n        window_size: int,\n        resampling_strategy: str = None,\n        patience: int = 3,\n        epochs: int = 1000,\n        batch_size: int = 256,\n        class_weight: Dict[int, float] = None\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Train and evaluate a given model.\n    \n    Parameters\n    ----------\n    model : Model\n        Model to be trained and evaluated.\n    \n    model_name : str\n        Name of the model.\n\n    region : str\n        The kind of regulatory region considered (enhancers or\n        promoters).\n\n    train_sequence : MixedSequence\n        Training set that will be used to train the model.\n\n    valid_sequence : MixedSequence\n        Validation set that will be used during training.\n\n    test_sequence : MixedSequence\n        Test set that will be used for the final evaluation\n        of the model.\n\n    holdout_number : int\n        Number of the current holdout iteration.\n\n    use_feature_selection : bool\n        Whether a feature selection algorithm has been applied\n        to the data.\n\n    use_validation_set : bool\n        Whether the validation set has been extracted from the\n        training set (True) or the test set is being used also\n        as validation set (False).\n\n    window_size : int\n        Size of the window used to sample the data. The parameter \n        is needed to allow the cache decorator to store different \n        values when using different window sizes.\n\n    resampling_strategy : str or None\n        The kind of resampling strategy (e.g. random \n        under-sampling, SMOTE over-sampling, etc.), if any, \n        applied to the data.\n\n    patience : int\n        Number of epochs to be used for early stopping.\n\n    epochs : int\n        Maximum number of epochs to be used for training.\n\n    batch_size : int\n        Size of the batches to be fed to the model.\n\n    class_weight : dict of str -> float or None\n        Optional dictionary mapping class indices (integers) \n        to a weight (float) value, used for weighting the loss \n        function during training only. May be beneficial for\n        imbalanced datasets.\n\n    Raises\n    ------\n    ValueError\n        If the region string is neither \"enhancers\" or \"promoters\".\n\n    Returns\n    ------- \n    A dictionary containing the training history and \n    a DataFrame containing the evaluation scores of the model \n    on both the training set and test set.\n    \"\"\"\n    supported_regions = [\"enhancers\", \"promoters\"]\n\n    if region not in supported_regions:\n        raise ValueError(\n            f\"Parameter 'region' should be one \"\n            f\"of {supported_regions}.\"\n            f\"Got '{region}' instead.\"\n        )\n\n    history = pd.DataFrame(model.fit(\n        train_sequence,\n        validation_data=valid_sequence,\n        epochs=epochs,\n        batch_size=batch_size,\n        verbose=False,\n        class_weight=class_weight,\n        callbacks=[\n            EarlyStopping(\"val_loss\", patience=patience),\n            TqdmCallback(verbose=1, tqdm_class=tqdm_notebook)\n        ]\n    ).history)\n\n    train_evaluation = dict(\n        zip(\n            model.metrics_names,\n            model.evaluate(\n                train_sequence,\n                verbose=False,\n                batch_size=batch_size\n            )\n        )\n    )\n    test_evaluation = dict(\n        zip(\n            model.metrics_names,\n            model.evaluate(\n                test_sequence,\n                verbose=False,\n                batch_size=batch_size\n            )\n        )\n    )\n    train_evaluation[\"run_type\"] = \"train\"\n    test_evaluation[\"run_type\"] = \"test\"\n    for evaluation in (train_evaluation, test_evaluation):\n        evaluation[\"model_name\"] = model_name\n        evaluation[\"region\"] = region\n        evaluation[\"holdout_number\"] = holdout_number\n        evaluation[\"use_feature_selection\"] = use_feature_selection\n        evaluation[\"use_validation_set\"] = use_validation_set\n        evaluation[\"patience\"] = patience\n        evaluation[\"max_epochs\"] = epochs\n        evaluation[\"resampling_strategy\"] = resampling_strategy\n        evaluation[\"class_weights\"] = class_weight is not None\n        evaluation[\"window_size\"] = window_size\n\n    evaluations = pd.DataFrame([\n        train_evaluation,\n        test_evaluation\n    ])\n\n    return history, evaluations\n",
    "backend_metadata": {
        "type": "pandas",
        "columns_types": {
            "loss": "float64",
            "accuracy": "float64",
            "recall": "float64",
            "precision": "float64",
            "AUROC": "float64",
            "AUPRC": "float64",
            "f1_score": "float64",
            "balanced_accuracy": "float64",
            "specificity": "float64",
            "miss_rate": "float64",
            "fall_out": "float64",
            "mcc": "float64",
            "tp/t": "float64",
            "fp/t": "float64",
            "tn/t": "float64",
            "fn/t": "float64",
            "negative_predictive_value": "float64",
            "false_discovery_rate": "float64",
            "false_omission_rate": "float64",
            "prevalence_threshold": "float64",
            "threat_score": "float64",
            "fowlkes_mallows_index": "float64",
            "informedness": "float64",
            "markedness": "float64",
            "LR_pos": "float64",
            "LR_neg": "float64",
            "DOR": "float64",
            "run_type": "str",
            "model_name": "str",
            "region": "str",
            "holdout_number": "int64",
            "use_feature_selection": "bool",
            "use_validation_set": "bool",
            "patience": "int64",
            "max_epochs": "int64",
            "resampling_strategy": "str",
            "class_weights": "bool",
            "window_size": "int64"
        },
        "index_type": "int64",
        "columns_names_type": "str"
    },
    "parameters": {
        "class_weight": null,
        "batch_size": 256,
        "epochs": 1000,
        "patience": 3,
        "resampling_strategy": null,
        "model_name": "mmnn_pre",
        "holdout_number": 2,
        "use_feature_selection": false,
        "use_validation_set": true,
        "window_size": 256,
        "region": "enhancers"
    }
}